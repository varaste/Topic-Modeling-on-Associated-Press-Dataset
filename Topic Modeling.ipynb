{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import zeros, int8, log\n",
    "from pylab import random\n",
    "import sys\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import jieba\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# segmentation, stopwords filtering and document-word matrix generating\n",
    "# [return]:\n",
    "# N : number of documents\n",
    "# M : length of dictionary\n",
    "# word2id : a map mapping terms to their corresponding ids\n",
    "# id2word : a map mapping ids to terms\n",
    "# X : document-word matrix, N*M, each line is the number of terms that show up in the document\n",
    "def preprocessing(datasetFilePath):\n",
    "\n",
    "      #stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # read the documents\n",
    "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
    "    documents = [document.strip() for document in file]\n",
    "    file.close()\n",
    "\n",
    "    # number of documents\n",
    "    N = len(documents)\n",
    "\n",
    "    wordCounts = [];\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    currentId = 0;\n",
    "\n",
    "    for document in documents:\n",
    "        segList = jieba.cut(document)\n",
    "        wordCount = {}\n",
    "        for word in segList:\n",
    "            word = word.lower().strip()\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stops:\n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word] = currentId;\n",
    "                    id2word[currentId] = word;\n",
    "                    currentId += 1;\n",
    "                if word in wordCount:\n",
    "                    wordCount[word] += 1\n",
    "                else:\n",
    "                    wordCount[word] = 1\n",
    "        wordCounts.append(wordCount);\n",
    "\n",
    "    # length of dictionary\n",
    "    M = len(word2id)\n",
    "\n",
    "    # generate the document-word matrix\n",
    "    X = zeros([N, M], int8)\n",
    "    for word in word2id.keys():\n",
    "        j = word2id[word]\n",
    "        for i in range(0, N):\n",
    "            if word in wordCounts[i]:\n",
    "                X[i, j] = wordCounts[i][word];\n",
    "\n",
    "\n",
    "    return N, M, word2id, id2word, X\n",
    "\n",
    "\n",
    "def initializeParameters():\n",
    "    for i in range(0, N):\n",
    "        normalization = sum(PI[i, :])\n",
    "        for j in range(0, K):\n",
    "            PI[i, j] /= normalization;\n",
    "\n",
    "    for i in range(0, K):\n",
    "        normalization = sum(theta[i, :])\n",
    "        for j in range(0, M):\n",
    "            theta[i, j] /= normalization;\n",
    "\n",
    "\n",
    "def EStep():\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            denominator = 0\n",
    "            for k in range(0, K):\n",
    "                p[i, j, k] = theta[k, j] * PI[i, k]\n",
    "                denominator += theta[k, j] * PI[i, k]\n",
    "            for k in range(0, K):\n",
    "                p[i, j, k] /= float(denominator)\n",
    "\n",
    "def MStep():\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        for j in range(0, M):\n",
    "            for i in range(0, N):\n",
    "                theta[k, j] += p[i, j, k] * X[i, j]\n",
    "                denominator += p[i, j, k] * X[i, j]\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] /= float(denominator)\n",
    "    for i in range(0, N):\n",
    "        denominator = 0\n",
    "        for k in range(0, K):\n",
    "            for j in range(0, M):\n",
    "                PI[i, k] += p[i, j, k] * X[i, j]\n",
    "                denominator += p[i, j, k] * X[i, j]\n",
    "        for k in range(0, K):\n",
    "            PI[i, k] /= float(denominator)\n",
    "\n",
    "\n",
    "# calculate the log likelihood\n",
    "def LogLikelihood():\n",
    "    loglikelihood = 0\n",
    "    for i in range(0, N):\n",
    "        loglikelihood += log(1/float(N))\n",
    "        for j in range(0, M):\n",
    "            p = 0\n",
    "            for k in range(0, K):\n",
    "                p += PI[i, k] * theta[k, j]\n",
    "            loglikelihood += log(p) * X[i, j]\n",
    "    return loglikelihood\n",
    "\n",
    "\n",
    "# calculate the log likelihood with Background\n",
    "def LogLikelihoodBG(lamb):\n",
    "\n",
    "    WordCount = sum(sum(X))\n",
    "\n",
    "    BG = zeros([M], int8)\n",
    "    for i in range(0,M):\n",
    "      wordInDocs = sum(X.T[i])\n",
    "      BG[i]=wordInDocs/WordCount\n",
    "\n",
    "    loglikelihood = 0\n",
    "    for i in range(0, N):\n",
    "        loglikelihood += log(1/float(N))\n",
    "        for j in range(0, M):\n",
    "            p = BG[j]*lamb\n",
    "            for k in range(0, K):\n",
    "                p += (PI[i, k] * theta[k, j]) * (1 - lamb)\n",
    "            loglikelihood += log(p) * X[i, j]\n",
    "    return loglikelihood\n",
    "\n",
    "\n",
    "# output the params of model and top words of topics to files\n",
    "def output():\n",
    "    # document-topic distribution\n",
    "    file = codecs.open(docTopicDist,'w','utf-8')\n",
    "    for i in range(0, N):\n",
    "        tmp = ''\n",
    "        for j in range(0, K):\n",
    "            tmp += str(PI[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    # topic-word distribution\n",
    "    file = codecs.open(topicWordDist,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        tmp = ''\n",
    "        for j in range(0, M):\n",
    "            tmp += str(theta[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    # dictionary\n",
    "    file = codecs.open(dictionary,'w','utf-8')\n",
    "    for i in range(0, M):\n",
    "        file.write(id2word[i] + '\\n')\n",
    "    file.close()\n",
    "\n",
    "    # top words of each topic\n",
    "    file = codecs.open(topicWords,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        topicword = []\n",
    "        ids = theta[i, :].argsort()\n",
    "        for j in ids:\n",
    "            topicword.insert(0, id2word[j])\n",
    "        tmp = ''\n",
    "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
    "            tmp += word + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set the default params and read the params from cmd\n",
    "datasetFilePath = \"dataset.txt\"\n",
    "K = 10   # number of topic\n",
    "maxIteration = 30\n",
    "threshold = 0.0001\n",
    "topicWordsNum = 10\n",
    "docTopicDist = \"docTopicDist.txt\"\n",
    "topicWordDist = 'topicWordDistribution.txt'\n",
    "dictionary = 'dictionary.dic'\n",
    "topicWords = \"topics.txt\"\n",
    "\n",
    "# preprocessing\n",
    "N, M, word2id, id2word, X = preprocessing(datasetFilePath)\n",
    "\n",
    "# theta[i, j] : p(zj|di): 2-D matrix\n",
    "PI = random([N, K])\n",
    "# beta[i, j] : p(wj|zi): 2-D matrix\n",
    "theta = random([K, M])\n",
    "# p[i, j, k] : p(zk|di,wj): 3-D tensor\n",
    "p = zeros([N, M, K])\n",
    "\n",
    "initializeParameters() # normarlize\n",
    "\n",
    "# EM algorithm\n",
    "oldLoglikelihood = 1\n",
    "newLoglikelihood = 1\n",
    "for i in range(0, maxIteration):\n",
    "    EStep() #implement E step\n",
    "    MStep() #implement M step\n",
    "    # newLoglikelihood = LogLikelihood()\n",
    "    newLoglikelihood = LogLikelihoodBG(0.3)\n",
    "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "    # you should see increasing loglikelihood\n",
    "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "        break\n",
    "    oldLoglikelihood = newLoglikelihood\n",
    "\n",
    "output()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}